{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDTM mapping assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline to automate Study Data Tabulation Model (SDTM) for mapping used for reporting of clinical studies results to the FDA in the US. This is a challenging task for biostatistians as it's effort-intensive, cognitively demanding, and requires full verification of accuracy and completeness of the SDTM dataset for regulatory compliance. In many cases this activity leads to fatigue and burnout. \n",
    "\n",
    "### What we'll cover\n",
    "\n",
    "1. Environment setup\n",
    "2. Study data generator - Since we're looking at CDMS as input, we'll ask GPT to generate CDMS synthetic dataset. \n",
    "3. Study explorer - Discover missing domains, missing fields, or formatting issues upfront.\n",
    "4. SDTM mapping agent - Dynamic automapper agent creating a draft mapping specification with trace/chain of thought record. \n",
    "In this case, we configure the agent for SDTMIG 3.3.\n",
    "5. SDTM validator agent - automatically verifies and updates the mappings. As with the previous steps, lineage/traceability of record changes is critical and should not be broken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "1.1 Install the necessary libraries\n",
    "1.2 Import the necessary libraries\n",
    "1.3 Set up API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/site-packages (1.75.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rmarkov/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rmarkov/Library/Python/3.10/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/rmarkov/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rmarkov/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/site-packages (1.75.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x108117550>: Failed to establish a new connection: [Errno 51] Network is unreachable')': /simple/openai/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x108117fa0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/openai/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x10814c340>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/openai/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x10814c4f0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /simple/openai/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/rmarkov/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Downloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.75.0\n",
      "    Uninstalling openai-1.75.0:\n",
      "      Successfully uninstalled openai-1.75.0\n",
      "Successfully installed openai-1.76.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn torch transformers matplotlib openai\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# First, create a `.env` file in the same directory as this file with the following content:\n",
    "# OPENAI_API_KEY=your_api_key_here\n",
    "# Then install the python-dotenv if you haven't\n",
    "# %pip install python-dotenv\n",
    "# Then run the following code block\n",
    "#########\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Specify path to the env file, in our case: `cred.env` file in the current directory\n",
    "load_dotenv(\"cred.env\",override=True)\n",
    "\n",
    "# Retrieve the API key and set it to openai_api_key variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client with the loaded key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Verify that the API key is loaded successfully\n",
    "print(\"API key loaded:\", bool(client.api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-audio-preview-2024-12-17\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "text-embedding-3-small\n",
      "o4-mini\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "o4-mini-2025-04-16\n",
      "gpt-4o-realtime-preview\n",
      "babbage-002\n",
      "gpt-4\n",
      "text-embedding-ada-002\n",
      "chatgpt-4o-latest\n",
      "text-embedding-3-large\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-audio-preview\n",
      "o1-preview-2024-09-12\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4.1-mini\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4.1-mini-2025-04-14\n",
      "davinci-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4o-search-preview\n",
      "gpt-4-turbo\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4-0125-preview\n",
      "gpt-4o-2024-11-20\n",
      "whisper-1\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-image-1\n",
      "o1-preview\n",
      "gpt-4-0613\n",
      "gpt-4.5-preview\n",
      "gpt-4.5-preview-2025-02-27\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "omni-moderation-2024-09-26\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "tts-1-hd\n",
      "gpt-4o\n",
      "tts-1-hd-1106\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4.1\n",
      "gpt-4o-transcribe\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-transcribe\n",
      "o1-mini\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-3.5-turbo-0125\n",
      "o1-2024-12-17\n",
      "o1-mini-2024-09-12\n",
      "o1\n",
      "o1-pro\n",
      "o1-pro-2025-03-19\n",
      "tts-1\n",
      "gpt-4-1106-preview\n",
      "gpt-4o-mini-tts\n",
      "tts-1-1106\n",
      "omni-moderation-latest\n"
     ]
    }
   ],
   "source": [
    "# Check available openAI models \n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Study data generator\n",
    "\n",
    "In this section we load the clinical data. Since we're looking at CDMS as input, we'll ask GPT to generate CDMS synthetic dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version_cdms_dataset = \"v1\"  # version of the prompt design. bump with each update\n",
    "\n",
    "prompt_cdms_dataset = \"\"\"\n",
    "You are a clinical data manager.\n",
    "\n",
    "Generate synthetic, realistic raw CDMS datasets for a clinical trial covering the following domains:\n",
    "- DM (Demographics)\n",
    "- VS (Vital Signs)\n",
    "- AE (Adverse Events)\n",
    "- EX (Exposure)\n",
    "- TS (Trial Summary)\n",
    "- TV (Trial Visits)\n",
    "\n",
    "Requirements:\n",
    "- Create realistic and complete sets of fields typically collected at the CDMS level based on your domain knowledge.\n",
    "- Do NOT limit the number of fields; reflect actual clinical trialdata collection practices and include diverse metadata (e.g., demographics, vital measurements, event severities, dosing routes).\n",
    "- Output each domain as a realistic **CSV format** block: a header row followed by data rows.\n",
    "- Each domain should include 10-20 records, multiple subjects, and multiple visits where appropriate.\n",
    "- Include missing/null values in some fields realistically.\n",
    "- Clearly label each section with \"DM Dataset:\", \"VS Dataset:\", \"AE Dataset:\", etc.\n",
    "\n",
    "Respond only with the CSV content clearly separated by domain - no explanations, introductions, or notes.\n",
    "\n",
    "Be as accurate and complete as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client (reads API key from env var or config)\n",
    "client = OpenAI()\n",
    "model_name = \"o4-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function wrapper to callGPT and get CDMS dataset,for a given prompt `prompt_cdms_dataset`\n",
    "def generate_cdms_dataset(prompt_cdms_dataset):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_cdms_dataset}],\n",
    "            # temperature=0.1, # option not available with latest reasoning models\n",
    "            # max_tokens=4000  # option not available with latest reasoning models\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating explanation: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CDMS dataset.\n",
      "DM Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,SUBJID,SITEID,RFSTDTC,RFENDTC,DMDTC,BRTHDTC,AGE,AGEU,SEX,RACE,ETHNIC,ARM,ARMCD,COUNTRY\n",
      "STUDY001,DM,STUDY001-101,101,SITE01,2022-01-10,2022-06-10,2022-01-05,1980-04-12,41,Y,M,White,Not Hispanic or Latino,Treatment A,A1,USA\n",
      "STUDY001,DM,STUDY001-102,102,SITE02,2022-02-15,2022-07-15,2022-02-10,1975-11-07,46,Y,F,Asian,Hispanic,Treatment B,B1,CAN\n",
      "STUDY001,DM,STUDY001-103,103,SITE01,2022-03-01,2022-08-01,2022-02-25,1990-06-20,31,Y,M,Black,Not Hispanic or Latino,Placebo,P1,USA\n",
      "STUDY001,DM,STUDY001-104,104,SITE03,2022-01-20,2022-06-20,2022-01-15,1985-01-30,37,Y,F,White,Hispanic,Treatment A,A1,GBR\n",
      "STUDY001,DM,STUDY001-105,105,SITE02,2022-04-05,2022-09-05,2022-04-01,1965-09-15,56,Y,M,Other,,Placebo,P1,AUS\n",
      "STUDY001,DM,STUDY001-106,106,SITE03,2022-05-10,2022-10-10,2022-05-05,1995-12-01,26,Y,F,Asian,Not Hispanic or Latino,Treatment B,B1,\n",
      "STUDY001,DM,STUDY001-107,107,SITE01,2022-06-15,2022-11-15,2022-06-10,1970-03-22,52,Y,F,White,Not Hispanic or Latino,Treatment A,A1,USA\n",
      "STUDY001,DM,STUDY001-108,108,SITE02,2022-07-01,2022-12-01,2022-06-28,1988-08-08,33,Y,M,Black,Not Hispanic or Latino,Placebo,P1,USA\n",
      "STUDY001,DM,STUDY001-109,109,SITE03,2022-03-12,2022-08-12,2022-03-10,1992-02-14,30,Y,F,Asian,Hispanic,Treatment B,B1,IND\n",
      "STUDY001,DM,STUDY001-110,110,SITE01,2022-04-18,2022-09-18,2022-04-15,1978-05-25,43,Y,M,White,Not Hispanic or Latino,Treatment A,A1,USA\n",
      "STUDY001,DM,STUDY001-111,111,SITE02,2022-05-20,2022-10-20,2022-05-18,1969-07-30,53,Y,F,Black,,Placebo,P1,USA\n",
      "STUDY001,DM,STUDY001-112,112,SITE03,2022-06-22,2022-11-22,2022-06-20,1982-10-05,39,Y,M,White,Not Hispanic or Latino,Treatment B,B1,GBR\n",
      "\n",
      "VS Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,VSSEQ,VISITNUM,VISIT,VSTESTCD,VSTEST,VSORRES,VSORRESU,VSBLFL,VSDTC\n",
      "STUDY001,VS,STUDY001-101,1,1,Visit 1,WT,Weight,70.2,kg,Y,2022-01-10\n",
      "STUDY001,VS,STUDY001-101,2,2,Visit 2,WT,Weight,70.5,kg,,2022-02-07\n",
      "STUDY001,VS,STUDY001-102,3,1,Visit 1,HT,Height,165,cm,Y,2022-02-15\n",
      "STUDY001,VS,STUDY001-102,4,2,Visit 2,HT,Height,166,cm,,2022-03-15\n",
      "STUDY001,VS,STUDY001-103,5,1,Visit 1,SBP,Systolic BP,120,mmHg,Y,2022-03-01\n",
      "STUDY001,VS,STUDY001-103,6,2,Visit 2,SBP,Systolic BP,118,mmHg,,2022-03-29\n",
      "STUDY001,VS,STUDY001-104,7,1,Visit 1,DBP,Diastolic BP,80,mmHg,Y,2022-01-20\n",
      "STUDY001,VS,STUDY001-104,8,2,Visit 2,DBP,Diastolic BP,82,mmHg,, \n",
      "STUDY001,VS,STUDY001-105,9,1,Visit 1,HR,Heart Rate,72,bpm,Y,2022-04-05\n",
      "STUDY001,VS,STUDY001-105,10,2,Visit 2,HR,Heart Rate,,bpm,,2022-05-03\n",
      "STUDY001,VS,STUDY001-106,11,1,Visit 1,RR,Respiration Rate,16,breaths/min,Y,2022-05-10\n",
      "STUDY001,VS,STUDY001-106,12,2,Visit 2,RR,Respiration Rate,18,breaths/min,,2022-06-07\n",
      "STUDY001,VS,STUDY001-107,13,1,Visit 1,TEMP,Temperature,36.7,C,Y,2022-06-15\n",
      "STUDY001,VS,STUDY001-107,14,2,Visit 2,TEMP,Temperature,37.1,C,,2022-07-13\n",
      "STUDY001,VS,STUDY001-108,15,1,Visit 1,WT,Weight,82.0,kg,Y,2022-07-01\n",
      "STUDY001,VS,STUDY001-108,16,2,Visit 2,WT,Weight,82.3,kg,,2022-07-29\n",
      "STUDY001,VS,STUDY001-109,17,1,Visit 1,SBP,Systolic BP,130,mmHg,Y,2022-03-12\n",
      "STUDY001,VS,STUDY001-109,18,2,Visit 2,SBP,Systolic BP,128,mmHg,,2022-04-09\n",
      "STUDY001,VS,STUDY001-110,19,1,Visit 1,HT,Height,172,cm,Y,2022-04-18\n",
      "STUDY001,VS,STUDY001-110,20,2,Visit 2,HT,Height,,cm,,2022-05-16\n",
      "\n",
      "AE Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,AESEQ,AETERM,AESTDTC,AEENDTC,AESEV,AESER,AEACN,AEOUT\n",
      "STUDY001,AE,STUDY001-101,1,Headache,2022-02-10,2022-02-12,Mild,No,Resumed at same dose,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-102,2,Nausea,2022-03-20,2022-03-22,Moderate,No,Medication withdrawn,Ongoing\n",
      "STUDY001,AE,STUDY001-103,3,Hypertension,2022-04-05,2022-04-10,Severe,Yes,Drug withdrawn,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-104,4,Back pain,2022-02-15,,Mild,No,None,Ongoing\n",
      "STUDY001,AE,STUDY001-105,5,Dizziness,2022-05-01,2022-05-03,Moderate,No,Trial drug dose reduced,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-106,6,Insomnia,2022-06-12,2022-06-19,Mild,No,None,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-107,7,Diarrhea,2022-07-02,2022-07-05,Moderate,No,Drug interrupted,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-108,8,Rash,2022-08-01,2022-08-10,Severe,Yes,Medication withdrawn,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-109,9,Anxiety,2022-04-20,,Mild,No,None,Ongoing\n",
      "STUDY001,AE,STUDY001-110,10,Fatigue,2022-05-30,2022-06-02,Moderate,No,Medication withheld,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-111,11,Pruritus,2022-06-05,2022-06-07,Mild,No,None,Recovered/resolved\n",
      "STUDY001,AE,STUDY001-112,12,Abdominal pain,2022-07-10,2022-07-15,Severe,Yes,Drug withdrawn,Fatal\n",
      "\n",
      "EX Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,EXSEQ,EXTRT,EXDOSE,EXDOSU,EXDOSFRQ,EXROUTE,EXSTDTC,EXENDTC\n",
      "STUDY001,EX,STUDY001-101,1,Treatment A,200,mg,QD,Oral,2022-01-11,2022-01-11\n",
      "STUDY001,EX,STUDY001-101,2,Treatment A,200,mg,QD,Oral,2022-02-11,2022-02-11\n",
      "STUDY001,EX,STUDY001-101,3,Treatment A,200,mg,QD,Oral,2022-03-11,2022-03-11\n",
      "STUDY001,EX,STUDY001-102,4,Treatment A,200,mg,QD,Oral,2022-02-16,2022-02-16\n",
      "STUDY001,EX,STUDY001-102,5,Treatment A,200,mg,QD,Oral,2022-03-16,2022-03-16\n",
      "STUDY001,EX,STUDY001-102,6,Treatment A,200,mg,QD,Oral,2022-04-16,2022-04-16\n",
      "STUDY001,EX,STUDY001-103,7,Treatment A,200,mg,QD,Oral,2022-03-02,2022-03-02\n",
      "STUDY001,EX,STUDY001-103,8,Treatment A,200,mg,QD,Oral,2022-04-02,2022-04-02\n",
      "STUDY001,EX,STUDY001-103,9,Treatment A,200,mg,QD,Oral,2022-05-02,2022-05-02\n",
      "STUDY001,EX,STUDY001-104,10,Treatment B,150,mg,QD,SC,2022-01-22,2022-01-22\n",
      "STUDY001,EX,STUDY001-104,11,Treatment B,150,mg,QD,SC,2022-02-22,2022-02-22\n",
      "STUDY001,EX,STUDY001-104,12,Treatment B,150,mg,QD,SC,2022-03-22,2022-03-22\n",
      "STUDY001,EX,STUDY001-105,13,Treatment A,200,mg,QD,Oral,2022-04-06,2022-04-06\n",
      "STUDY001,EX,STUDY001-105,14,Treatment A,200,mg,QD,Oral,2022-05-06,2022-05-06\n",
      "STUDY001,EX,STUDY001-105,15,Treatment A,200,mg,QD,Oral,2022-06-06,2022-06-06\n",
      "STUDY001,EX,STUDY001-106,16,Treatment B,150,mg,QD,SC,2022-05-11,2022-05-11\n",
      "STUDY001,EX,STUDY001-106,17,Treatment B,150,mg,QD,SC,2022-06-11,2022-06-11\n",
      "STUDY001,EX,STUDY001-106,18,Treatment B,150,mg,QD,SC,2022-07-11,\n",
      "\n",
      "TS Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,TSSEQ,TSPARMCD,TSPARM,TSVAL,TSVALU\n",
      "STUDY001,TS,STUDY001-101,1,TRTDAYS,Treatment Duration,59,Days\n",
      "STUDY001,TS,STUDY001-102,2,TRTDAYS,Treatment Duration,60,Days\n",
      "STUDY001,TS,STUDY001-103,3,TRTDAYS,Treatment Duration,61,Days\n",
      "STUDY001,TS,STUDY001-104,4,TRTDAYS,Treatment Duration,60,Days\n",
      "STUDY001,TS,STUDY001-105,5,TRTDAYS,Treatment Duration,62,Days\n",
      "STUDY001,TS,STUDY001-106,6,TRTDAYS,Treatment Duration,,Days\n",
      "STUDY001,TS,STUDY001-107,7,TRTDAYS,Treatment Duration,67,Days\n",
      "STUDY001,TS,STUDY001-108,8,TRTDAYS,Treatment Duration,65,Days\n",
      "STUDY001,TS,STUDY001-109,9,TRTDAYS,Treatment Duration,70,Days\n",
      "STUDY001,TS,STUDY001-110,10,TRTDAYS,Treatment Duration,55,Days\n",
      "STUDY001,TS,STUDY001-111,11,TRTDAYS,Treatment Duration,58,Days\n",
      "STUDY001,TS,STUDY001-112,12,TRTDAYS,Treatment Duration,45,Days\n",
      "\n",
      "TV Dataset:\n",
      "STUDYID,DOMAIN,USUBJID,VISITNUM,VISIT,TVSTDTC\n",
      "STUDY001,TV,STUDY001-101,1,Screening,2022-01-05\n",
      "STUDY001,TV,STUDY001-101,2,Baseline,2022-01-10\n",
      "STUDY001,TV,STUDY001-102,1,Screening,2022-02-10\n",
      "STUDY001,TV,STUDY001-102,2,Baseline,2022-02-15\n",
      "STUDY001,TV,STUDY001-103,1,Screening,2022-02-25\n",
      "STUDY001,TV,STUDY001-103,2,Baseline,2022-03-01\n",
      "STUDY001,TV,STUDY001-104,1,Screening,2022-01-15\n",
      "STUDY001,TV,STUDY001-104,2,Baseline,2022-01-20\n",
      "STUDY001,TV,STUDY001-105,1,Screening,\n",
      "STUDY001,TV,STUDY001-105,2,Baseline,2022-04-05\n",
      "STUDY001,TV,STUDY001-106,1,Screening,2022-05-05\n",
      "STUDY001,TV,STUDY001-106,2,Baseline,2022-05-10\n",
      "STUDY001,TV,STUDY001-107,1,Screening,2022-06-10\n",
      "STUDY001,TV,STUDY001-107,2,Baseline,2022-06-15\n",
      "STUDY001,TV,STUDY001-108,1,Screening,2022-06-28\n",
      "STUDY001,TV,STUDY001-108,2,Baseline,2022-07-01\n",
      "STUDY001,TV,STUDY001-109,1,Screening,2022-03-10\n",
      "STUDY001,TV,STUDY001-109,2,Baseline,2022-03-12\n",
      "STUDY001,TV,STUDY001-110,1,Screening,2022-04-15\n",
      "STUDY001,TV,STUDY001-110,2,Baseline,2022-04-18\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT response with CDMS dataset\n",
    "cdms_csv_text = generate_cdms_dataset(prompt_cdms_dataset)\n",
    "print(\"Generated CDMS dataset.\")\n",
    "print(cdms_csv_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as new study folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def create_new_cdms_study_folder(base_dir=\"cdms_data\"):\n",
    "    # Make sure base directory exists\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all existing study folders\n",
    "    existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    pattern = re.compile(r\"study_(\\d+)\")\n",
    "    \n",
    "    numbers = []\n",
    "    for d in existing:\n",
    "        match = pattern.match(d)\n",
    "        if match:\n",
    "            numbers.append(int(match.group(1)))\n",
    "    \n",
    "    # Determine next number\n",
    "    if numbers:\n",
    "        next_number = max(numbers) + 1\n",
    "    else:\n",
    "        next_number = 1\n",
    "    \n",
    "    # New folder name\n",
    "    new_folder = f\"study_{next_number}\"\n",
    "    new_folder_path = os.path.join(base_dir, new_folder)\n",
    "    \n",
    "    # Create the folder\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Created new study folder: {new_folder_path}\")\n",
    "    return new_folder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new study folder: cdms_data/study_1\n"
     ]
    }
   ],
   "source": [
    "new_study_path = create_new_cdms_study_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "def save_gpt_csvs_to_study_folder(gpt_text_response, study_folder_path):\n",
    "    # Make sure the target folder exists\n",
    "    os.makedirs(study_folder_path, exist_ok=True)\n",
    "    \n",
    "    domains = [\"DM\", \"VS\", \"AE\", \"EX\", \"TS\", \"TV\"]\n",
    "    \n",
    "    for domain in domains:\n",
    "        start_marker = f\"{domain} Dataset:\"\n",
    "        try:\n",
    "            start_idx = gpt_text_response.index(start_marker) + len(start_marker)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: {domain} Dataset not found in the GPT response.\")\n",
    "            continue\n",
    "\n",
    "        # Find next domain start to separate sections\n",
    "        next_domain_idx = len(gpt_text_response)\n",
    "        for d in domains:\n",
    "            if d != domain and f\"{d} Dataset:\" in gpt_text_response:\n",
    "                idx = gpt_text_response.index(f\"{d} Dataset:\")\n",
    "                if idx > start_idx:\n",
    "                    next_domain_idx = min(next_domain_idx, idx)\n",
    "\n",
    "        # Extract CSV text for the domain\n",
    "        csv_text = gpt_text_response[start_idx:next_domain_idx].strip()\n",
    "\n",
    "        # Load CSV text into Pandas DataFrame\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(csv_text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {domain} CSV: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save CSV to the study folder\n",
    "        file_path = os.path.join(study_folder_path, f\"{domain.lower()}.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {domain} data to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DM data to cdms_data/study_1/dm.csv\n",
      "Saved VS data to cdms_data/study_1/vs.csv\n",
      "Saved AE data to cdms_data/study_1/ae.csv\n",
      "Saved EX data to cdms_data/study_1/ex.csv\n",
      "Saved TS data to cdms_data/study_1/ts.csv\n",
      "Saved TV data to cdms_data/study_1/tv.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the latest generated dataset into the folder\n",
    "save_gpt_csvs_to_study_folder(cdms_csv_text, new_study_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability also store the raw model response in a file\n",
    "def save_raw_gpt_response(gpt_text_response, study_folder_path):\n",
    "    file_path = os.path.join(study_folder_path, \"raw_response.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gpt_text_response)\n",
    "    print(f\"Saved raw GPT response to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw GPT response to cdms_data/study_1/raw_response.txt\n"
     ]
    }
   ],
   "source": [
    "save_raw_gpt_response(cdms_csv_text, new_study_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For traceability also save prompt\n",
    "def save_prompt(prompt_text, study_folder_path):\n",
    "    file_path = os.path.join(study_folder_path, \"prompt.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(prompt_text)\n",
    "    print(f\"Saved GPT prompt to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GPT prompt to cdms_data/study_1/prompt.txt\n"
     ]
    }
   ],
   "source": [
    "save_prompt(prompt_cdms_dataset, new_study_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_metadata(study_folder_path, model_name, prompt_version):\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"prompt_version\": prompt_version,\n",
    "        \"gpt_model\": model_name,\n",
    "        \"number_of_subjects\": None\n",
    "    }\n",
    "    \n",
    "    dm_file_path = os.path.join(study_folder_path, \"dm.csv\")\n",
    "    if os.path.exists(dm_file_path):\n",
    "        try:\n",
    "            dm = pd.read_csv(dm_file_path)\n",
    "            \n",
    "            # Only accept SUBJID\n",
    "            if \"SUBJID\" in dm.columns:\n",
    "                metadata[\"number_of_subjects\"] = dm[\"SUBJID\"].nunique()\n",
    "                print(f\"Found {metadata['number_of_subjects']} unique subjects using 'SUBJID'.\")\n",
    "            elif \"subjid\" in dm.columns:\n",
    "                metadata[\"number_of_subjects\"] = dm[\"subjid\"].nunique()\n",
    "                print(f\"Found {metadata['number_of_subjects']} unique subjects using 'subjid'.\")\n",
    "            else:\n",
    "                print(f\"No SUBJID field found in DM dataset. Cannot determine number of subjects.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DM dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"DM dataset not found at {dm_file_path}. Skipping subject count.\")\n",
    "\n",
    "    file_path = os.path.join(study_folder_path, \"metadata.json\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"Saved metadata to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 unique subjects using 'SUBJID'.\n",
      "Saved metadata to cdms_data/study_1/metadata.json\n"
     ]
    }
   ],
   "source": [
    "save_metadata(new_study_path, prompt_version_cdms_dataset, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Study explorer\n",
    "Biostatistians need an easy way to browse, select, and explore studies and what sort of SDTM mappings will be needed based on electronic data capture (EDC) source data. \n",
    "\n",
    "Before mapping, scientists need to understand the data. They may discover missing domains, missing fields, or formatting issues upfront."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show available studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available studies:\n",
      "1. study_1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "BASE_DIR = \"cdms_data\"\n",
    "\n",
    "def list_studies(base_dir=BASE_DIR):\n",
    "    studies = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    return studies\n",
    "\n",
    "# List studies\n",
    "studies = list_studies()\n",
    "print(\"Available studies:\")\n",
    "for idx, study in enumerate(studies, 1):\n",
    "    print(f\"{idx}. {study}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: study_1\n"
     ]
    }
   ],
   "source": [
    "# Pick a study Id to explore\n",
    "selected_idx = 0\n",
    "\n",
    "selected_study = studies[selected_idx]\n",
    "selected_study_path = os.path.join(BASE_DIR, selected_study)\n",
    "\n",
    "print(f\"Selected: {selected_study}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets for the selected study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_study_datasets(study_path):\n",
    "    datasets = {}\n",
    "    domains = [\"dm\", \"vs\", \"ae\", \"ex\", \"ts\", \"tv\"]\n",
    "    \n",
    "    for domain in domains:\n",
    "        file_path = os.path.join(study_path, f\"{domain}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            datasets[domain.upper()] = pd.read_csv(file_path)\n",
    "        else:\n",
    "            print(f\"{domain}.csv not found in {study_path}\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 48 fields across study domains.\n",
      "['AEACN', 'AEENDTC', 'AEOUT', 'AESEQ', 'AESER', 'AESEV', 'AESTDTC', 'AETERM', 'AGE', 'AGEU', 'ARM', 'ARMCD', 'BRTHDTC', 'COUNTRY', 'DMDTC', 'DOMAIN', 'ETHNIC', 'EXDOSE', 'EXDOSFRQ', 'EXDOSU', 'EXENDTC', 'EXROUTE', 'EXSEQ', 'EXSTDTC', 'EXTRT', 'RACE', 'RFENDTC', 'RFSTDTC', 'SEX', 'SITEID', 'STUDYID', 'SUBJID', 'TSPARM', 'TSPARMCD', 'TSSEQ', 'TSVAL', 'TSVALU', 'TVSTDTC', 'USUBJID', 'VISIT', 'VISITNUM', 'VSBLFL', 'VSDTC', 'VSORRES', 'VSORRESU', 'VSSEQ', 'VSTEST', 'VSTESTCD']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "datasets = load_study_datasets(selected_study_path)\n",
    "\n",
    "# Collect all fields across all domains\n",
    "cdms_fields = set()\n",
    "\n",
    "for domain, df in datasets.items():\n",
    "    cdms_fields.update(df.columns.tolist())\n",
    "\n",
    "# Optional: Sort fields for nicer display\n",
    "cdms_fields = sorted(list(cdms_fields))\n",
    "\n",
    "print(f\"Collected {len(cdms_fields)} fields across study domains.\")\n",
    "print(cdms_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show basic dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Study Summary:\n",
      "\n",
      "Domain: DM\n",
      "- Records: 12\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'SUBJID', 'SITEID', 'RFSTDTC', 'RFENDTC', 'DMDTC', 'BRTHDTC', 'AGE', 'AGEU', 'SEX', 'RACE', 'ETHNIC', 'ARM', 'ARMCD', 'COUNTRY']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>SUBJID</th>\n",
       "      <th>SITEID</th>\n",
       "      <th>RFSTDTC</th>\n",
       "      <th>RFENDTC</th>\n",
       "      <th>DMDTC</th>\n",
       "      <th>BRTHDTC</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGEU</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RACE</th>\n",
       "      <th>ETHNIC</th>\n",
       "      <th>ARM</th>\n",
       "      <th>ARMCD</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>DM</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>101</td>\n",
       "      <td>SITE01</td>\n",
       "      <td>2022-01-10</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>1980-04-12</td>\n",
       "      <td>41</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Treatment A</td>\n",
       "      <td>A1</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>DM</td>\n",
       "      <td>STUDY001-102</td>\n",
       "      <td>102</td>\n",
       "      <td>SITE02</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>1975-11-07</td>\n",
       "      <td>46</td>\n",
       "      <td>Y</td>\n",
       "      <td>F</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Treatment B</td>\n",
       "      <td>B1</td>\n",
       "      <td>CAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>DM</td>\n",
       "      <td>STUDY001-103</td>\n",
       "      <td>103</td>\n",
       "      <td>SITE01</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>1990-06-20</td>\n",
       "      <td>31</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>Black</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>P1</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  SUBJID  SITEID     RFSTDTC     RFENDTC  \\\n",
       "0  STUDY001     DM  STUDY001-101     101  SITE01  2022-01-10  2022-06-10   \n",
       "1  STUDY001     DM  STUDY001-102     102  SITE02  2022-02-15  2022-07-15   \n",
       "2  STUDY001     DM  STUDY001-103     103  SITE01  2022-03-01  2022-08-01   \n",
       "\n",
       "        DMDTC     BRTHDTC  AGE AGEU SEX   RACE                  ETHNIC  \\\n",
       "0  2022-01-05  1980-04-12   41    Y   M  White  Not Hispanic or Latino   \n",
       "1  2022-02-10  1975-11-07   46    Y   F  Asian                Hispanic   \n",
       "2  2022-02-25  1990-06-20   31    Y   M  Black  Not Hispanic or Latino   \n",
       "\n",
       "           ARM ARMCD COUNTRY  \n",
       "0  Treatment A    A1     USA  \n",
       "1  Treatment B    B1     CAN  \n",
       "2      Placebo    P1     USA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain: VS\n",
      "- Records: 20\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'VSSEQ', 'VISITNUM', 'VISIT', 'VSTESTCD', 'VSTEST', 'VSORRES', 'VSORRESU', 'VSBLFL', 'VSDTC']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>VSSEQ</th>\n",
       "      <th>VISITNUM</th>\n",
       "      <th>VISIT</th>\n",
       "      <th>VSTESTCD</th>\n",
       "      <th>VSTEST</th>\n",
       "      <th>VSORRES</th>\n",
       "      <th>VSORRESU</th>\n",
       "      <th>VSBLFL</th>\n",
       "      <th>VSDTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>VS</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Visit 1</td>\n",
       "      <td>WT</td>\n",
       "      <td>Weight</td>\n",
       "      <td>70.2</td>\n",
       "      <td>kg</td>\n",
       "      <td>Y</td>\n",
       "      <td>2022-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>VS</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Visit 2</td>\n",
       "      <td>WT</td>\n",
       "      <td>Weight</td>\n",
       "      <td>70.5</td>\n",
       "      <td>kg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>VS</td>\n",
       "      <td>STUDY001-102</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Visit 1</td>\n",
       "      <td>HT</td>\n",
       "      <td>Height</td>\n",
       "      <td>165.0</td>\n",
       "      <td>cm</td>\n",
       "      <td>Y</td>\n",
       "      <td>2022-02-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  VSSEQ  VISITNUM    VISIT VSTESTCD  VSTEST  \\\n",
       "0  STUDY001     VS  STUDY001-101      1         1  Visit 1       WT  Weight   \n",
       "1  STUDY001     VS  STUDY001-101      2         2  Visit 2       WT  Weight   \n",
       "2  STUDY001     VS  STUDY001-102      3         1  Visit 1       HT  Height   \n",
       "\n",
       "   VSORRES VSORRESU VSBLFL       VSDTC  \n",
       "0     70.2       kg      Y  2022-01-10  \n",
       "1     70.5       kg    NaN  2022-02-07  \n",
       "2    165.0       cm      Y  2022-02-15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain: AE\n",
      "- Records: 12\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'AESEQ', 'AETERM', 'AESTDTC', 'AEENDTC', 'AESEV', 'AESER', 'AEACN', 'AEOUT']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>AESEQ</th>\n",
       "      <th>AETERM</th>\n",
       "      <th>AESTDTC</th>\n",
       "      <th>AEENDTC</th>\n",
       "      <th>AESEV</th>\n",
       "      <th>AESER</th>\n",
       "      <th>AEACN</th>\n",
       "      <th>AEOUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>AE</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>1</td>\n",
       "      <td>Headache</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>2022-02-12</td>\n",
       "      <td>Mild</td>\n",
       "      <td>No</td>\n",
       "      <td>Resumed at same dose</td>\n",
       "      <td>Recovered/resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>AE</td>\n",
       "      <td>STUDY001-102</td>\n",
       "      <td>2</td>\n",
       "      <td>Nausea</td>\n",
       "      <td>2022-03-20</td>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>No</td>\n",
       "      <td>Medication withdrawn</td>\n",
       "      <td>Ongoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>AE</td>\n",
       "      <td>STUDY001-103</td>\n",
       "      <td>3</td>\n",
       "      <td>Hypertension</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>2022-04-10</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Drug withdrawn</td>\n",
       "      <td>Recovered/resolved</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  AESEQ        AETERM     AESTDTC     AEENDTC  \\\n",
       "0  STUDY001     AE  STUDY001-101      1      Headache  2022-02-10  2022-02-12   \n",
       "1  STUDY001     AE  STUDY001-102      2        Nausea  2022-03-20  2022-03-22   \n",
       "2  STUDY001     AE  STUDY001-103      3  Hypertension  2022-04-05  2022-04-10   \n",
       "\n",
       "      AESEV AESER                 AEACN               AEOUT  \n",
       "0      Mild    No  Resumed at same dose  Recovered/resolved  \n",
       "1  Moderate    No  Medication withdrawn             Ongoing  \n",
       "2    Severe   Yes        Drug withdrawn  Recovered/resolved  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain: EX\n",
      "- Records: 18\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'EXSEQ', 'EXTRT', 'EXDOSE', 'EXDOSU', 'EXDOSFRQ', 'EXROUTE', 'EXSTDTC', 'EXENDTC']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>EXSEQ</th>\n",
       "      <th>EXTRT</th>\n",
       "      <th>EXDOSE</th>\n",
       "      <th>EXDOSU</th>\n",
       "      <th>EXDOSFRQ</th>\n",
       "      <th>EXROUTE</th>\n",
       "      <th>EXSTDTC</th>\n",
       "      <th>EXENDTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>EX</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>1</td>\n",
       "      <td>Treatment A</td>\n",
       "      <td>200</td>\n",
       "      <td>mg</td>\n",
       "      <td>QD</td>\n",
       "      <td>Oral</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>2022-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>EX</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>2</td>\n",
       "      <td>Treatment A</td>\n",
       "      <td>200</td>\n",
       "      <td>mg</td>\n",
       "      <td>QD</td>\n",
       "      <td>Oral</td>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>2022-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>EX</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>3</td>\n",
       "      <td>Treatment A</td>\n",
       "      <td>200</td>\n",
       "      <td>mg</td>\n",
       "      <td>QD</td>\n",
       "      <td>Oral</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  EXSEQ        EXTRT  EXDOSE EXDOSU EXDOSFRQ  \\\n",
       "0  STUDY001     EX  STUDY001-101      1  Treatment A     200     mg       QD   \n",
       "1  STUDY001     EX  STUDY001-101      2  Treatment A     200     mg       QD   \n",
       "2  STUDY001     EX  STUDY001-101      3  Treatment A     200     mg       QD   \n",
       "\n",
       "  EXROUTE     EXSTDTC     EXENDTC  \n",
       "0    Oral  2022-01-11  2022-01-11  \n",
       "1    Oral  2022-02-11  2022-02-11  \n",
       "2    Oral  2022-03-11  2022-03-11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain: TS\n",
      "- Records: 12\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'TSSEQ', 'TSPARMCD', 'TSPARM', 'TSVAL', 'TSVALU']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>TSSEQ</th>\n",
       "      <th>TSPARMCD</th>\n",
       "      <th>TSPARM</th>\n",
       "      <th>TSVAL</th>\n",
       "      <th>TSVALU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TS</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>1</td>\n",
       "      <td>TRTDAYS</td>\n",
       "      <td>Treatment Duration</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TS</td>\n",
       "      <td>STUDY001-102</td>\n",
       "      <td>2</td>\n",
       "      <td>TRTDAYS</td>\n",
       "      <td>Treatment Duration</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TS</td>\n",
       "      <td>STUDY001-103</td>\n",
       "      <td>3</td>\n",
       "      <td>TRTDAYS</td>\n",
       "      <td>Treatment Duration</td>\n",
       "      <td>61.0</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  TSSEQ TSPARMCD              TSPARM  TSVAL  \\\n",
       "0  STUDY001     TS  STUDY001-101      1  TRTDAYS  Treatment Duration   59.0   \n",
       "1  STUDY001     TS  STUDY001-102      2  TRTDAYS  Treatment Duration   60.0   \n",
       "2  STUDY001     TS  STUDY001-103      3  TRTDAYS  Treatment Duration   61.0   \n",
       "\n",
       "  TSVALU  \n",
       "0   Days  \n",
       "1   Days  \n",
       "2   Days  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain: TV\n",
      "- Records: 20\n",
      "- Columns: ['STUDYID', 'DOMAIN', 'USUBJID', 'VISITNUM', 'VISIT', 'TVSTDTC']\n",
      "- Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>VISITNUM</th>\n",
       "      <th>VISIT</th>\n",
       "      <th>TVSTDTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TV</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>1</td>\n",
       "      <td>Screening</td>\n",
       "      <td>2022-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TV</td>\n",
       "      <td>STUDY001-101</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>2022-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDY001</td>\n",
       "      <td>TV</td>\n",
       "      <td>STUDY001-102</td>\n",
       "      <td>1</td>\n",
       "      <td>Screening</td>\n",
       "      <td>2022-02-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STUDYID DOMAIN       USUBJID  VISITNUM      VISIT     TVSTDTC\n",
       "0  STUDY001     TV  STUDY001-101         1  Screening  2022-01-05\n",
       "1  STUDY001     TV  STUDY001-101         2   Baseline  2022-01-10\n",
       "2  STUDY001     TV  STUDY001-102         1  Screening  2022-02-10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_study_summary(datasets):\n",
    "    print(\"\\nStudy Summary:\")\n",
    "\n",
    "    for domain, df in datasets.items():\n",
    "        print(f\"\\nDomain: {domain}\")\n",
    "        print(f\"- Records: {len(df)}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(\"- Sample records:\")\n",
    "        display(df.head(3))\n",
    "\n",
    "# Show the summary\n",
    "show_study_summary(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a summary string of the dataset\n",
    "def generate_study_description(datasets):\n",
    "    desc = \"\"\n",
    "    for domain, df in datasets.items():\n",
    "        desc += f\"\\nDomain: {domain}\\n\"\n",
    "        desc += f\"Fields: {list(df.columns)}\\n\"\n",
    "        desc += f\"Sample (first 3 rows):\\n{df.head(3).to_string(index=False)}\\n\"\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate long string describing the study datasets\n",
    "study_description = generate_study_description(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt for gpt to review study long string\n",
    "prompt_study_review = f\"\"\"\n",
    "You are a clinical data quality assistant.\n",
    "\n",
    "You are given CDMS datasets from a clinical study, with the end goal of creating SDTMIG-conformant mapping and analysis dataset for submission to the FDA.  \n",
    "Your task is to review the quality of source data for later conversion to SDTMIG-compliant dataset. Please perform the following:\n",
    "\n",
    "1. Summarize the study datasets briefly (domains, number of fields, sample data quality).\n",
    "2. Identify potential data issues (e.g., wrong formatting, missing values, unexpected field names).\n",
    "3. Suggest corrections or improvements (e.g., standardize visit codes, fix missing dates).\n",
    "\n",
    "Datasets:\n",
    "\n",
    "{study_description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_review(prompt_study_review):\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_study_review}],\n",
    "            #temperature=0.2,   # not an option with latest reasoning models\n",
    "            #max_tokens=1500    # not anoption with latest reasoning models\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reviewing study: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Study Dataset Summary  \n",
      "• Domains and field counts:  \n",
      "  – DM (Demographics): 16 fields  \n",
      "  – VS (Vital Signs): 12 fields  \n",
      "  – AE (Adverse Events): 10 fields  \n",
      "  – EX (Exposure): 11 fields  \n",
      "  – TS (Trial Summary – derived): 8 fields  \n",
      "  – TV (Trial Visits): 6 fields  \n",
      "\n",
      "• Overall sample quality:  \n",
      "  – Dates are in ISO “YYYY-MM-DD” format throughout.  \n",
      "  – Subjects are consistently identified by USUBJID.  \n",
      "  – Numeric results in VS and TS are given with units.  \n",
      "  – Controlled terminology is used in many fields (e.g., AESEV, EXDOSFRQ), but some values deviate slightly from CDISC standards.  \n",
      "\n",
      "2. Potential Data Issues  \n",
      "\n",
      "Domain DM  \n",
      "  • AGEU uses “Y” instead of full CDISC term “Years.”  \n",
      "  • RACE and ETHNIC values are free-text (“White,” “Not Hispanic or Latino,” “Hispanic”) rather than CDISC controlled terms (e.g., WHITE, ASIAN, NOT HISPANIC OR LATINO).  \n",
      "  • RFSTDTC (First treatment) post-dates DMDTC (Demography date) in row 1; confirm DMDTC = screening date.  \n",
      "\n",
      "Domain VS  \n",
      "  • VSBLFL (Baseline flag) is missing (NaN) on non-baseline rows rather than coded “N.”  \n",
      "  • No numeric standard result (VSSTRESN) derived from VSORRES.  \n",
      "  • Visit naming: “Visit 1,” “Visit 2” vs TV domain “Screening,” “Baseline” – may not align.  \n",
      "\n",
      "Domain AE  \n",
      "  • AEACN (“Action taken”) and AEOUT (“Outcome”) use mixed free text (e.g., “Resumed at same dose,” “Recovered/resolved,” “Ongoing”) – may need mapping to CDISC standard terms or codes.  \n",
      "  • AEENDTC present for all, but confirm no missing AESTDTC/AEENDTC.  \n",
      "\n",
      "Domain EX  \n",
      "  • EXDOSFRQ = “QD” is acceptable but confirm use of CDISC code list (e.g., QD = once daily).  \n",
      "  • EXROUTE = “Oral” vs CDISC “ORAL” – case consistency.  \n",
      "\n",
      "Domain TS  \n",
      "  • TSPARMCD = TRTDAYS is correct; confirm that TSSEQ is sequential per subject.  \n",
      "\n",
      "Domain TV  \n",
      "  • VISITNUM = 1 for “Screening” – CDISC convention often uses 0 for Screening.  \n",
      "  • Ensure VISIT names in TV match VISIT in VS.  \n",
      "\n",
      "3. Suggested Corrections and Improvements  \n",
      "\n",
      "Domain DM  \n",
      "  – Standardize AGEU to “Years.”  \n",
      "  – Map RACE to CDISC controlled terminology (e.g., WHITE, BLACK OR AFRICAN AMERICAN, ASIAN).  \n",
      "  – Map ETHNIC to “HISPANIC OR LATINO” / “NOT HISPANIC OR LATINO.”  \n",
      "  – Validate that DMDTC ≤ RFSTDTC for all subjects; if DMDTC represents screening date, align it with TV domain.  \n",
      "\n",
      "Domain VS  \n",
      "  – Populate VSBLFL with “Y” for baseline visit(s) and “N” for all others.  \n",
      "  – Derive VSSTRESN as numeric version of VSORRES (70.2 → 70.2, etc.) and VSSTRESU = VSORRESU.  \n",
      "  – Standardize VISIT names to exactly match TV (e.g., use “Screening,” “Baseline,” “Visit 2”).  \n",
      "  – Ensure VISITNUM in VS matches TV VISITNUM.  \n",
      "\n",
      "Domain AE  \n",
      "  – Map AEACN to CDISC “AEACNOTH” code list (e.g., “DRUG WITHDRAWN,” “DOSE REDUCED,” “DOSE NOT CHANGED”).  \n",
      "  – Map AEOUT to CDISC “AEOUT” code list (e.g., “RECOVERED/RESOLVED,” “ONGOING”).  \n",
      "  – Check for any missing AESTDTC/AEENDTC and populate or impute as per SDTM rules.  \n",
      "\n",
      "Domain EX  \n",
      "  – Standardize EXROUTE to uppercase “ORAL.”  \n",
      "  – Confirm EXDOSFRQ values against CDISC code list for dosing frequency (QD, BID, etc.).  \n",
      "  – Ensure EXSTDTC/EXENDTC cover the full date-time per SDTM (add time if available).  \n",
      "\n",
      "Domain TS  \n",
      "  – Verify TSSEQ is unique and sequential within each USUBJID.  \n",
      "  – Consider adding additional TSPARMCD parameters as needed (e.g., CUMDOSE).  \n",
      "\n",
      "Domain TV  \n",
      "  – Re-code VISITNUM = 0 for “Screening” per CDISC convention, or document the custom scheme.  \n",
      "  – Align TV VISIT labels exactly with VS and other domains.  \n",
      "  – Ensure every scheduled visit for each subject has an entry (no missing visits).  \n",
      "\n",
      "General  \n",
      "  – Perform a global check for unexpected field names or extra blank columns.  \n",
      "  – Run standard SDTM validation (e.g., Pinnacle 21) to catch formatting, label, type, and controlled-term issues prior to mapping.  \n",
      "\n",
      "Implementing these corrections will improve source-to-SDTM mapping consistency, ensure controlled terminology compliance, and reduce FDA review queries.\n"
     ]
    }
   ],
   "source": [
    "# Generate study review for our study\n",
    "gpt_review = study_review(prompt_study_review)\n",
    "print(gpt_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SDTM mapping agent\n",
    "Dynamic automapper agent creating a draft mapping specification with trace/chain of thought record. \n",
    "In this case, we configure the agent for SDTMIG 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_field_mappings(fields, model_name=\"gpt-4o-mini\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert SDTMIG 3.3 mapping assistant.\n",
    "\n",
    "You will be given a list of CDMS source variables. For each source variable:\n",
    "- Identify the target SDTMIG 3.3 standard variable it should map to.\n",
    "- Indicate if a transformation is needed (e.g., format change, controlled terminology mapping, case normalization).\n",
    "- Provide a detailed Chain of Thought (CoT) explanation for why the mapping was made and any transformations suggested.\n",
    "\n",
    "Respond in a structured JSONL format: one line per field, with the following keys:\n",
    "- source_field\n",
    "- target_field\n",
    "- transformation_needed (true/false)\n",
    "- transformation_description\n",
    "- draft_reasoning\n",
    "\n",
    "Here is the list of fields:\n",
    "{fields}\n",
    "\n",
    "Start when ready.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Error reviewing study: {e}\"\n",
    "    \n",
    "    result = response.choices[0].message.content.strip()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```jsonl\n",
      "{\"source_field\":\"AEACN\",\"target_field\":\"AEACN\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AEACN directly maps to the corresponding SDTM variable without any transformation.\"}\n",
      "{\"source_field\":\"AEENDTC\",\"target_field\":\"AEENDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AEENDTC is already formatted to match SDTM requirements and does not require any changes.\"}\n",
      "{\"source_field\":\"AEOUT\",\"target_field\":\"AEOUT\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AEOUT aligns directly with SDTM variables; no transformation is necessary.\"}\n",
      "{\"source_field\":\"AESEQ\",\"target_field\":\"AESEQ\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AESEQ is kept as is as it directly corresponds to SDTM.\"}\n",
      "{\"source_field\":\"AESER\",\"target_field\":\"AESER\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AESER does not require change; it is compatible with SDTM.\"}\n",
      "{\"source_field\":\"AESEV\",\"target_field\":\"AESEV\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AESEV maps directly to SDTM without needing transformations.\"}\n",
      "{\"source_field\":\"AESTDTC\",\"target_field\":\"AESTDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AESTDTC is adequately formatted for SDTM and requires no changes.\"}\n",
      "{\"source_field\":\"AETERM\",\"target_field\":\"AETERM\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AETERM directly matches the SDTM naming convention, no transformation needed.\"}\n",
      "{\"source_field\":\"AGE\",\"target_field\":\"AGE\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AGE can be used directly as it conforms to SDTM requirements.\"}\n",
      "{\"source_field\":\"AGEU\",\"target_field\":\"AGEU\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"AGEU is a standard unit field that maps directly to SDTM.\"}\n",
      "{\"source_field\":\"ARM\",\"target_field\":\"ARM\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"ARM corresponds to SDTM without requiring any alterations.\"}\n",
      "{\"source_field\":\"ARMCD\",\"target_field\":\"ARMCD\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"ARMCD directly maps to SDTM with no transformations necessary.\"}\n",
      "{\"source_field\":\"BRTHDTC\",\"target_field\":\"BRTHDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"BRTHDTC is formatted correctly for SDTM and does not need changes.\"}\n",
      "{\"source_field\":\"COUNTRY\",\"target_field\":\"COUNTRY\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"COUNTRY aligns directly with SDTM; no transformation needed.\"}\n",
      "{\"source_field\":\"DMDTC\",\"target_field\":\"DMDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"DMDTC does not need changes and aligns directly with SDTM.\"}\n",
      "{\"source_field\":\"DOMAIN\",\"target_field\":\"DOMAIN\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"DOMAIN maps directly as it is already aligned with SDTM definitions.\"}\n",
      "{\"source_field\":\"ETHNIC\",\"target_field\":\"ETHNIC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"ETHNIC is directly usable in SDTM without transformations.\"}\n",
      "{\"source_field\":\"EXDOSE\",\"target_field\":\"EXDOSE\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXDOSE can be used directly as it conforms to SDTM requirements.\"}\n",
      "{\"source_field\":\"EXDOSFRQ\",\"target_field\":\"EXDOSFRQ\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXDOSFRQ directly matches SDTM variable definitions.\"}\n",
      "{\"source_field\":\"EXDOSU\",\"target_field\":\"EXDOSU\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXDOSU is a standard unit field that maps directly to SDTM.\"}\n",
      "{\"source_field\":\"EXENDTC\",\"target_field\":\"EXENDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXENDTC is formatted correctly for SDTM requirements.\"}\n",
      "{\"source_field\":\"EXROUTE\",\"target_field\":\"EXROUTE\",\"transformation_needed\":true,\"transformation_description\":\"Requires controlled terminology for route of administration.\",\"draft_reasoning\":\"EXROUTE must adhere to controlled terminology as outlined in SDTMIG.\"}\n",
      "{\"source_field\":\"EXSEQ\",\"target_field\":\"EXSEQ\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXSEQ aligns directly with SDTM without slight transformation.\"}\n",
      "{\"source_field\":\"EXSTDTC\",\"target_field\":\"EXSTDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXSTDTC directly maps to the corresponding SDTM variable with no changes needed.\"}\n",
      "{\"source_field\":\"EXTRT\",\"target_field\":\"EXTRT\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"EXTRT is already aligned with SDTM variable definitions.\"}\n",
      "{\"source_field\":\"RACE\",\"target_field\":\"RACE\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"RACE is directly mappable to SDTM with no transformation.\"}\n",
      "{\"source_field\":\"RFENDTC\",\"target_field\":\"RFENDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"RFENDTC maps directly to SDTM without needing any changes.\"}\n",
      "{\"source_field\":\"RFSTDTC\",\"target_field\":\"RFSTDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"RFSTDTC is conformant with SDTM specifications and does not need modifications.\"}\n",
      "{\"source_field\":\"SEX\",\"target_field\":\"SEX\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"SEX maps directly to SDTM definitions and is not transformed.\"}\n",
      "{\"source_field\":\"SITEID\",\"target_field\":\"SITEID\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"SITEID is consistent with SDTM definitions and does not require transformation.\"}\n",
      "{\"source_field\":\"STUDYID\",\"target_field\":\"STUDYID\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"STUDYID directly matches SDTM with no transformation needed.\"}\n",
      "{\"source_field\":\"SUBJID\",\"target_field\":\"SUBJID\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"SUBJID is formatted correctly for SDTM and requires no changes.\"}\n",
      "{\"source_field\":\"TSPARM\",\"target_field\":\"TSPARM\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TSPARM directly corresponds to SDTM definitions.\"}\n",
      "{\"source_field\":\"TSPARMCD\",\"target_field\":\"TSPARMCD\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TSPARMCD aligns with SDTM without needing changes.\"}\n",
      "{\"source_field\":\"TSSEQ\",\"target_field\":\"TSSEQ\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TSSEQ directly maps to SDTM.\"}\n",
      "{\"source_field\":\"TSVAL\",\"target_field\":\"TSVAL\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TSVAL is compliant with standard SDTM formats.\"}\n",
      "{\"source_field\":\"TSVALU\",\"target_field\":\"TSVALU\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TSVALU can be directly used as it conforms to SDTM definitions.\"}\n",
      "{\"source_field\":\"TVSTDTC\",\"target_field\":\"TVSTDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"TVSTDTC maps directly to SDTM and requires no changes.\"}\n",
      "{\"source_field\":\"USUBJID\",\"target_field\":\"USUBJID\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"USUBJID follows SDTM naming conventions accurately without the need for transformation.\"}\n",
      "{\"source_field\":\"VISIT\",\"target_field\":\"VISIT\",\"transformation_needed\":true,\"transformation_description\":\"May require controlled terminology mapping for visit names.\",\"draft_reasoning\":\"VISIT may need to be aligned with predefined controlled terminology as specified by SDTM.\"}\n",
      "{\"source_field\":\"VISITNUM\",\"target_field\":\"VISITNUM\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VISITNUM directly corresponds to SDTM without needing any change.\"}\n",
      "{\"source_field\":\"VSBLFL\",\"target_field\":\"VSBLFL\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSBLFL mapping does not require changes as it aligns with SDTM.\"}\n",
      "{\"source_field\":\"VSDTC\",\"target_field\":\"VSDTC\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSDTC conforms with SDTM variable format.\"}\n",
      "{\"source_field\":\"VSORRES\",\"target_field\":\"VSORRES\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSORRES is directly usable and matches SDTM definitions.\"}\n",
      "{\"source_field\":\"VSORRESU\",\"target_field\":\"VSORRESU\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSORRESU also directly matches SDTM's standard.\"}\n",
      "{\"source_field\":\"VSSEQ\",\"target_field\":\"VSSEQ\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSSEQ directly maps to the corresponding SDTM variable.\"}\n",
      "{\"source_field\":\"VSTEST\",\"target_field\":\"VSTEST\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSTEST is used directly in SDTM without transformation.\"}\n",
      "{\"source_field\":\"VSTESTCD\",\"target_field\":\"VSTESTCD\",\"transformation_needed\":false,\"transformation_description\":\"\",\"draft_reasoning\":\"VSTESTCD directly matches the SDTM definitions.\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Generate field mappings\n",
    "mapping_output_text = generate_field_mappings(cdms_fields)\n",
    "\n",
    "# Print for quick review\n",
    "print(mapping_output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save mapping trace directly into the selected study folder (e.g., cdms_data/study_1/mapping_draft.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mapping_jsonl(mapping_text, study_folder):\n",
    "    output_path = os.path.join(study_folder, \"mapping_draft.jsonl\")\n",
    "    lines = mapping_text.split(\"\\n\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "    print(f\"Mapping trace saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping trace saved to cdms_data/study_1/mapping_draft.jsonl\n"
     ]
    }
   ],
   "source": [
    "save_mapping_jsonl(mapping_output_text, selected_study_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interim results\n",
    "\n",
    "**Steps performed**\n",
    "For a given study datasets, we:\n",
    "1. load supported domains (DM, VS, AE, EX, TS, TV)\n",
    "2. explore data\tand note observations/irregularities in the data\n",
    "3. collect CDMS source fields dynamically\n",
    "4. generate initial SDTM mappings including source-to-target variable mapping, confirm whether transformation of values is needed, and CoT reasoning\n",
    "5. save mapping trace as mapping_draft.jsonl inside the selected study folder\n",
    "\n",
    "**Result**\n",
    "* A full, machine-readable, human-readable draft mapping\n",
    "* For every source field\n",
    "* With Chain of Thought explainability\n",
    "* Ready for validation\n",
    "\n",
    "**Benefit**\n",
    "Biostatisticians can review this draft as opposed to generating themselves from scratch.\n",
    "They can also update this workflow to include their existing transformation functions. \n",
    "That way the AI agent can identify the function needed for mapping, or suggest transformation code.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Mapping was performed as expected. The agent was able to parse different value fields (i.e., is more flexible and less maintenance effort than hardcoded automation & rule-based system). However, it was also evident that the generated source dataset hallucinated target variables in the source dataset which reduced the number of needed transformations in the mapping process. The mapping summary is as follows:\n",
    "\n",
    "|Category\t                                    |Examples\t                                 |Count         |\n",
    "|-----------------------------------------------|--------------------------------------------|--------------|\n",
    "|Direct, no transformation\t                    |STUDYID, USUBJID, SEX, AGE, SITEID\t         |Majority (~90%)|\n",
    "|Needs controlled terminology mapping\t        |EXROUTE, VISIT, ETHNIC, RACE, possibly AGEU |~5–10 fields  |\n",
    "|Needs date reformatting check (hidden risk)\t|AESTDTC, EXSTDTC, BRTHDTC\t                 |~5 fields (could be a silent miss)|\n",
    "\n",
    "**Needs revision**\n",
    "\n",
    "|Issue                  |Details\t                                        |Example                    |\n",
    "|-----------------------|---------------------------------------------------|---------------------------|\n",
    "|Overconfident          |No-Transformation                                  |Some fields like ETHNIC, RACE, and AGEU are marked as no-transformation, but in real SDTMIG, they often need controlled terminology mapping.\"White\" ➔ \"WHITE\", \"Not Hispanic\" ➔ \"NOT HISPANIC OR LATINO\", etc.|\n",
    "|Generic reasonings\t    |Some CoT draft reasonings are a bit shallow (e.g., \"FIELD maps directly with no changes\"), without checking value distributions.\t                         |Would want the validator agent to expand these.|\n",
    "|Formatting assumptions |GPT assumed that all date fields (e.g., AESTDTC, EXSTDTC, BRTHDTC) are properly formatted without transformation needed. In practice might not be true.     |E.g., even date fields may need reformatting to ISO 8601.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SDTM mapping validator\n",
    "The results from the mapping so far showed a few flaws that need revision step. \n",
    "Instead of letting the biostatistician operate on the draft, let's create a validator which automatically verifies and updates the mappings. As with the previous steps, lineage/traceability of record changes is critical and should not be broken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping draft stored locally in order to then parse draft mappings\n",
    "def load_mapping_draft(study_folder_path):\n",
    "    mapping_file = os.path.join(study_folder_path, \"mapping_draft.jsonl\")\n",
    "    mappings = []\n",
    "    with open(mapping_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    mappings.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON decode error: {e} on line: {line}\")\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define mapping validator agent\n",
    "def validate_mapping_entry(mapping_entry, model_name=\"gpt-4o\"):\n",
    "    prompt = f\"\"\"\n",
    "You are a senior SDTMIG 3.3 mapping validator.\n",
    "\n",
    "You will review a proposed mapping between a CDMS source field and an SDTM target field.\n",
    "\n",
    "Here is the draft mapping:\n",
    "- Source Field: {mapping_entry['source_field']}\n",
    "- Target Field: {mapping_entry['target_field']}\n",
    "- Transformation Needed: {mapping_entry['transformation_needed']}\n",
    "- Transformation Description: {mapping_entry['transformation_description']}\n",
    "- Draft Reasoning: {mapping_entry['draft_reasoning']}\n",
    "\n",
    "Your task:\n",
    "- Confirm whether the source_field should map to the target_field (if not, propose a correction).\n",
    "- Confirm whether a transformation is needed (if not accurate, correct it).\n",
    "- Expand or improve the Chain of Thought (CoT) reasoning.\n",
    "- Be stricter: require controlled terminology mapping if necessary (e.g., RACE, ETHNIC, AGEU).\n",
    "- Output a structured JSON object with:\n",
    "  - source_field\n",
    "  - validated_target_field\n",
    "  - transformation_needed (true/false)\n",
    "  - improved_transformation_description\n",
    "  - improved_reasoning\n",
    "  - validation_comments\n",
    "\n",
    "Respond ONLY with valid JSON, no triple backticks nor markdown formatting.\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        # Check if response is non-empty\n",
    "        validated_output = response.choices[0].message.content.strip()\n",
    "        # Simple clean if response has triple backticks\n",
    "        validated_output = validated_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        if not validated_output:\n",
    "            print(f\"Empty response for field {mapping_entry['source_field']}\")\n",
    "            return None\n",
    "\n",
    "        # Parse JSON safely\n",
    "        validated_json = json.loads(validated_output)\n",
    "        return validated_json\n",
    "\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"OpenAI API error for field {mapping_entry['source_field']}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for field {mapping_entry['source_field']}: {e}\")\n",
    "        print(f\"Response content: {validated_output}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for field {mapping_entry['source_field']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all mappings\n",
    "def validate_all_mappings(mappings, study_folder_path, model_name=\"gpt-4o\"):\n",
    "    validated_mappings = []\n",
    "    \n",
    "    for idx, mapping_entry in enumerate(mappings):\n",
    "        print(f\"Validating field {idx+1}/{len(mappings)}: {mapping_entry['source_field']}...\")\n",
    "        validated_mapping = validate_mapping_entry(mapping_entry, model_name)\n",
    "        \n",
    "        if validated_mapping:\n",
    "            validated_mappings.append(validated_mapping)\n",
    "        else:\n",
    "            print(f\"Skipping {mapping_entry['source_field']} due to validation failure.\")\n",
    "    \n",
    "    # Save validated mappings as `mapping_validated.jsonl` file\n",
    "    output_file = os.path.join(study_folder_path, \"mapping_validated.jsonl\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in validated_mappings:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    print(f\"Validated mappings saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0) on line: ```jsonl\n",
      "JSON decode error: Expecting value: line 1 column 1 (char 0) on line: ```\n",
      "Validating field 1/48: AEACN...\n",
      "Validating field 2/48: AEENDTC...\n",
      "Validating field 3/48: AEOUT...\n",
      "Validating field 4/48: AESEQ...\n",
      "Validating field 5/48: AESER...\n",
      "Validating field 6/48: AESEV...\n",
      "Validating field 7/48: AESTDTC...\n",
      "Validating field 8/48: AETERM...\n",
      "Validating field 9/48: AGE...\n",
      "Validating field 10/48: AGEU...\n",
      "Validating field 11/48: ARM...\n",
      "Validating field 12/48: ARMCD...\n",
      "Validating field 13/48: BRTHDTC...\n",
      "Validating field 14/48: COUNTRY...\n",
      "Validating field 15/48: DMDTC...\n",
      "Validating field 16/48: DOMAIN...\n",
      "Validating field 17/48: ETHNIC...\n",
      "Validating field 18/48: EXDOSE...\n",
      "Validating field 19/48: EXDOSFRQ...\n",
      "Validating field 20/48: EXDOSU...\n",
      "Validating field 21/48: EXENDTC...\n",
      "Validating field 22/48: EXROUTE...\n",
      "Validating field 23/48: EXSEQ...\n",
      "Validating field 24/48: EXSTDTC...\n",
      "Validating field 25/48: EXTRT...\n",
      "Validating field 26/48: RACE...\n",
      "Validating field 27/48: RFENDTC...\n",
      "Validating field 28/48: RFSTDTC...\n",
      "Validating field 29/48: SEX...\n",
      "Validating field 30/48: SITEID...\n",
      "Validating field 31/48: STUDYID...\n",
      "Validating field 32/48: SUBJID...\n",
      "Validating field 33/48: TSPARM...\n",
      "Validating field 34/48: TSPARMCD...\n",
      "Validating field 35/48: TSSEQ...\n",
      "Validating field 36/48: TSVAL...\n",
      "Validating field 37/48: TSVALU...\n",
      "Validating field 38/48: TVSTDTC...\n",
      "Validating field 39/48: USUBJID...\n",
      "Validating field 40/48: VISIT...\n",
      "Validating field 41/48: VISITNUM...\n",
      "Validating field 42/48: VSBLFL...\n",
      "Validating field 43/48: VSDTC...\n",
      "Validating field 44/48: VSORRES...\n",
      "Validating field 45/48: VSORRESU...\n",
      "Validating field 46/48: VSSEQ...\n",
      "Validating field 47/48: VSTEST...\n",
      "Validating field 48/48: VSTESTCD...\n",
      "Validated mappings saved to cdms_data/study_1/mapping_validated.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load draft mappings\n",
    "draft_mappings = load_mapping_draft(selected_study_path)\n",
    "\n",
    "# Validate them all\n",
    "validate_all_mappings(draft_mappings, selected_study_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interim results\n",
    "\n",
    "Draft mapping was already compliant for structural fields and dates (e.g., names, ISO8601) where no controlled terminology is needed.\n",
    "\n",
    "**Results**\n",
    "* The values inside fields need controlled terminology mapping (e.g., free-text → CDISC code).\n",
    "    Interpretation: Validator is stricter on value-level compliance, not just field names.\n",
    "* Field names were wrong in draft. Validator correctly rewired the mapping to match SDTMIG.\n",
    "    Interpretation: Validator catches not only terminology issues but structural mismatches.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "|Category\t                            |Count\t    |Example Fields                       |\n",
    "|---------------------------------------|-----------|-------------------------------------|\n",
    "|No transformation needed (confirmed)\t|~60%\t    |AEENDTC, AESTDTC, AESEQ, BRTHDTC, DMDTC, SITEID, SUBJID, VSSEQ, STUDYID, RFSTDTC      |\n",
    "|Controlled terminology required\t    |~35%\t    |RACE, ETHNIC, AEACN, AEOUT, AESEV, AESER, AGEU, EXROUTE, SEX, VISIT, EXDOSFRQ, EXTRT |\n",
    "|Field remapping required               |~5%\t    |TSPARM → TSPARMCD, VSTEST → VSTESTCD |\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The mapping validator agent successfully demonstrates:\n",
    "* AI model understanding of SDTM rules with sufficient precision e.g., when controlled terminology must be applied\n",
    "* Detailed CoT reasoning per field similar to human-like reasoning\n",
    "* SDTMIG 3.3-compliant guidance\n",
    "* Audit-ready traceability\n",
    "\n",
    "**Areas of further improvement**\n",
    "* Practical transformation guidance - ground recommendations in specific set of *predefined transformation functions* (direct_move, lookup from codelist for values not compliant with CDISC, assign, ISO8601)\n",
    "* Make output more structured to simplify integration with follow-on traditional automation steps, especially by introducing *issue categorization*, listing *correct and incorrect values*, and introducing *codelists/picklists* for internal coding (e.g., CAN=CANADA; 1=Mild,2=Moderate,3=Severe, etc.)\n",
    "\n",
    "Next, let's enhance our validator and compare output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SDTM codelists as Python dictionary. This should include controlled terminology and can be supplied from an external system if such metadata is already stored. \n",
    "codelists = {\n",
    "    \"AEACN\": [\"DOSE REDUCED\", \"DRUG INTERRUPTED\", \"DRUG WITHDRAWN\", \"NOT APPLICABLE\", \"UNKNOWN\"],\n",
    "    \"AEOUT\": [\"FATAL\", \"NOT RECOVERED/NOT RESOLVED\", \"RECOVERED/RESOLVED\", \"RECOVERED/RESOLVED WITH SEQUELAE\", \"RECOVERING/RESOLVING\", \"UNKNOWN\"],\n",
    "    \"AESEV\": [\"MILD\", \"MODERATE\", \"SEVERE\"],\n",
    "    \"AESER\": [\"Y\", \"N\"],\n",
    "    \"ETHNIC\": [\"HISPANIC OR LATINO\", \"NOT HISPANIC OR LATINO\", \"UNKNOWN\", \"NOT REPORTED\"],\n",
    "    \"RACE\": [\"AMERICAN INDIAN OR ALASKA NATIVE\", \"ASIAN\", \"BLACK OR AFRICAN AMERICAN\", \"NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER\", \"WHITE\", \"OTHER\", \"MULTIPLE\"],\n",
    "    \"COUNTRY\": [\"USA\", \"GBR\", \"DEU\", \"FRA\", \"ITA\", \"ESP\"],  # Example: ISO-3166-1 alpha-3 country codes\n",
    "    \"EXROUTE\": [\"ORAL\", \"INTRAVENOUS\", \"SUBCUTANEOUS\", \"TOPICAL\", \"INHALATION\"],\n",
    "    \"SEX\": [\"M\", \"F\", \"U\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced validator\n",
    "def validate_mapping_entry_enhanced(mapping_entry, model_name=\"gpt-4o\"):\n",
    "    # Get allowed values if the target field is in codelists\n",
    "    allowed_values = codelists.get(mapping_entry['target_field'], [])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an SDTM 3.3 clinical data validator agent.\n",
    "\n",
    "You are reviewing a draft mapping from a clinical CDMS source field to an SDTM target field.\n",
    "\n",
    "Here is the draft mapping:\n",
    "- Source Field: {mapping_entry['source_field']}\n",
    "- Draft Target Field: {mapping_entry['target_field']}\n",
    "- Draft Transformation Needed: {mapping_entry['transformation_needed']}\n",
    "- Draft Reasoning: {mapping_entry['draft_reasoning']}\n",
    "\n",
    "Allowed controlled terminology for the target field {mapping_entry['target_field']}:\n",
    "{allowed_values}\n",
    "\n",
    "Your job is to validate it and produce a highly structured output with the following fields:\n",
    "\n",
    "- source_field (str)\n",
    "- validated_target_field (str)\n",
    "- transformation_needed (bool)\n",
    "- transformation_function (one of [\"direct_move\", \"lookup_from_codelist\", \"assign_constant\", \"format_to_ISO8601\", \"derive_sequence\"])\n",
    "- issue_category (one of [\"Controlled Terminology Mapping\", \"Date Format Issue\", \"Structural Field Mapping\", \"Value Transformation\", \"No Issue\"])\n",
    "- incorrect_values (list of examples if applicable)\n",
    "- correct_values (list of examples if applicable)\n",
    "- allowed_values_codelist (list, if applicable, else empty)\n",
    "- improved_transformation_description (str)\n",
    "- improved_reasoning (str)\n",
    "- validation_comments (str)\n",
    "\n",
    "Rules:\n",
    "- Suggest a transformation function if needed.\n",
    "- If no transformation is needed, use \"direct_move\" and \"No Issue\".\n",
    "- If the target field uses controlled terminology, validate values against the allowed list.\n",
    "- Otherwise, leave allowed_values_codelist empty.\n",
    "\n",
    "IMPORTANT: Respond ONLY with a single pure JSON object. Do NOT wrap it inside triple backticks or markdown formatting.\n",
    "\n",
    "STRICT COMPLIANCE RULES: \n",
    "- Always validate mappings strictly according to the SDTMIG 3.3 standard.\n",
    "- Always validate fields against SDTMIG 3.3 standards. Do NOT invent new mappings. \n",
    "- For COUNTRY, use full country names only. For ETHNIC, reject any non-ethnicity terms.\n",
    "- Do not assume mappings based on ISO codes, clinical standards, or other common sense unless explicitly required by SDTMIG 3.3.\n",
    "- All mappings must align strictly with the provided SDTM controlled terminology.\n",
    "- If you are unsure whether a controlled terminology is specified in SDTMIG 3.3 for a field, assume NO transformation is needed.\n",
    "- Your job is to enforce SDTMIG 3.3 compliance, not general best practices.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        validated_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        validated_output = validated_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        if not validated_output:\n",
    "            print(f\"Empty response for field {mapping_entry['source_field']}\")\n",
    "            return None\n",
    "\n",
    "        validated_json = json.loads(validated_output)\n",
    "        return validated_json\n",
    "\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"OpenAI API error for field {mapping_entry['source_field']}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error for field {mapping_entry['source_field']}: {e}\")\n",
    "        print(f\"Response content: {validated_output}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for field {mapping_entry['source_field']}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating field 1/48: AEACN...\n",
      "Validating field 2/48: AEENDTC...\n",
      "Validating field 3/48: AEOUT...\n",
      "Validating field 4/48: AESEQ...\n",
      "Validating field 5/48: AESER...\n",
      "Validating field 6/48: AESEV...\n",
      "Validating field 7/48: AESTDTC...\n",
      "Validating field 8/48: AETERM...\n",
      "Validating field 9/48: AGE...\n",
      "Validating field 10/48: AGEU...\n",
      "Validating field 11/48: ARM...\n",
      "Validating field 12/48: ARMCD...\n",
      "Validating field 13/48: BRTHDTC...\n",
      "Validating field 14/48: COUNTRY...\n",
      "Validating field 15/48: DMDTC...\n",
      "Validating field 16/48: DOMAIN...\n",
      "Validating field 17/48: ETHNIC...\n",
      "Validating field 18/48: EXDOSE...\n",
      "Validating field 19/48: EXDOSFRQ...\n",
      "Validating field 20/48: EXDOSU...\n",
      "Validating field 21/48: EXENDTC...\n",
      "Validating field 22/48: EXROUTE...\n",
      "Validating field 23/48: EXSEQ...\n",
      "Validating field 24/48: EXSTDTC...\n",
      "Validating field 25/48: EXTRT...\n",
      "Validating field 26/48: RACE...\n",
      "Validating field 27/48: RFENDTC...\n",
      "Validating field 28/48: RFSTDTC...\n",
      "Validating field 29/48: SEX...\n",
      "Validating field 30/48: SITEID...\n",
      "Validating field 31/48: STUDYID...\n",
      "Validating field 32/48: SUBJID...\n",
      "Validating field 33/48: TSPARM...\n",
      "Validating field 34/48: TSPARMCD...\n",
      "Validating field 35/48: TSSEQ...\n",
      "Validating field 36/48: TSVAL...\n",
      "Validating field 37/48: TSVALU...\n",
      "Validating field 38/48: TVSTDTC...\n",
      "Validating field 39/48: USUBJID...\n",
      "Validating field 40/48: VISIT...\n",
      "Validating field 41/48: VISITNUM...\n",
      "Validating field 42/48: VSBLFL...\n",
      "Validating field 43/48: VSDTC...\n",
      "Validating field 44/48: VSORRES...\n",
      "Validating field 45/48: VSORRESU...\n",
      "Validating field 46/48: VSSEQ...\n",
      "Validating field 47/48: VSTEST...\n",
      "Validating field 48/48: VSTESTCD...\n",
      "Enhanced validated mappings saved to cdms_data/study_1/mapping_validated_enhanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "validated_mappings = []\n",
    "\n",
    "for idx, mapping_entry in enumerate(draft_mappings):\n",
    "    print(f\"Validating field {idx+1}/{len(draft_mappings)}: {mapping_entry['source_field']}...\")\n",
    "    validated_entry = validate_mapping_entry_enhanced(mapping_entry)\n",
    "    if validated_entry:\n",
    "        validated_mappings.append(validated_entry)\n",
    "    else:\n",
    "        print(f\"Skipped {mapping_entry['source_field']} due to validation error.\")\n",
    "\n",
    "# Save output\n",
    "enhanced_output_file = os.path.join(selected_study_path, \"mapping_validated_enhanced.jsonl\")\n",
    "with open(enhanced_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in validated_mappings:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Enhanced validated mappings saved to {enhanced_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "\n",
    "**Summary**  \n",
    "We now have:\n",
    "* Structured schema that machines can act on (build actual ETL pipelines)\n",
    "* Traceable audit trail that a human reviewer can follow\n",
    "* Clear agent handoff system (draft ➔ validator ➔ transformation executor)\n",
    "* Diagnostic report capability (how many fields needed what types of fixes)\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "|Aspect                     |Evaluation                                                 |\n",
    "|---------------------------|-----------------------------------------------------------|\n",
    "|Traceability\t            |Full (source ➔ target ➔ reason ➔ function)                 |\n",
    "|Explainability             |includes chain of thought for each record/variable         |\n",
    "|Potential for Autocoding\t|Structured output easy to pipe into code generation        |\n",
    "\n",
    "**Areas of improvement**\n",
    "\n",
    "There were a few notable gaps (now fixed) in previous iterations/prior to tweaking the prompt/instructions to remain compliant with SDTMIG 3.3. This underscores the importance of explicit ruleset definition and supply with the prompt. Among others, the controlled terminology from SDTMIG should be supplied as ruleset. \n",
    "\n",
    "|Field\t                    |Issue\t                                                    |Suggested Correction               |\n",
    "|---------------------------|-----------------------------------------------------------|-----------------------------------|\n",
    "|ETHNIC\t                    |Minor mistake: the examples show \"ASIAN\" and \"BLACK\" mapping to \"HISPANIC OR LATINO\" / \"NOT HISPANIC OR LATINO\" — which is wrong.\t| \"ASIAN\" and \"BLACK\" belong to RACE, not ETHNIC. ETHNIC values should be like \"HISPANIC OR LATINO\", \"NOT HISPANIC OR LATINO\", etc. If \"ASIAN\" or \"BLACK\" shows up, it's a field mismatch or source system error. Flag it as invalid for ETHNIC.|\n",
    "|COUNTRY allowed_values_codelist | The validator enlists a codelist which was not supplied by the user : [\"USA\", \"GBR\", \"DEU\", \"FRA\", \"ITA\", \"ESP\"] — these are ISO country codes. | Enforce compliance with SDTMIG3.3 and ONLY the provided codelists.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future directions \n",
    "This is the end of our notebook. You may want to continue the experimentation in any of the following directions:\n",
    "1. define list of allowed transformation functions which the agent should reference in the mapping specification to ensure strict compliance with your codebase (e.g., direct_move, get_usubjid, suppqual)\n",
    "2. rework the synthetic data generator which currently hallucinates SDTMIG output variables in the CDMS source dataset\n",
    "3. create a migration agent which allows to migrate a dataset to the next version of SDTMIG (in our example we used SDTMIG 3.3 so you could create a migration agent which automatically detects and remediates non-compliance with SDTMIG 3.4)\n",
    "4. Compare responses with another model such as Anthropic Claude and foundation models trained on clinical data \n",
    "5. Build a transformation engine that take the enhanced mapping specification. Assuming the specification has been reviewed by a human in the loop, the engine performs the data transformations for each variable as specified in the source-to-target mapping by invoking the function referenced in the mapping specification e.g., *direct_move* or *lookup_from_codelist*. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
